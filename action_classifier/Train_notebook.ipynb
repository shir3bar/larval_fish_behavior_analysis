{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3b9fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slowfast.datasets.ptv_datasets import PTVDatasetWrapper, PackPathway, DictToTuple\n",
    "from slowfast.config.defaults import assert_and_infer_cfg\n",
    "\n",
    "from slowfast.utils.misc import launch_job\n",
    "from slowfast.utils.parser import load_config, parse_args\n",
    "from pytorchvideo.data import (\n",
    "    LabeledVideoDataset,\n",
    "    make_clip_sampler,\n",
    ")\n",
    "from torch.utils.data import (\n",
    "    RandomSampler\n",
    ")\n",
    "from pytorchvideo.data.labeled_video_paths import LabeledVideoPaths\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    RandomShortSideScale,\n",
    "    ShortSideScale,\n",
    "    UniformCropVideo,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, RandomApply\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    NormalizeVideo,\n",
    "    RandomCropVideo,\n",
    "    RandomHorizontalFlipVideo,\n",
    "\n",
    ")\n",
    "import torch\n",
    "from slowfast.utils.metrics import topk_accuracies,topks_correct\n",
    "import torch.nn.functional as F\n",
    "import slowfast.visualization.tensorboard_vis as tb\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from slowfast.utils import metrics\n",
    "import numpy as np\n",
    "from slowfast.datasets import loader\n",
    "from slowfast.datasets.transform import RandomColorJitter, RandomGaussianBlur, RandomVerticalFlipVideo, RandomRot90Video, VarianceImageTransform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4de7527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def div255(x):\n",
    "    \"\"\"\n",
    "    Scale clip frames from [0, 255] to [0, 1].\n",
    "    Args:\n",
    "        x (Tensor): A tensor of the clip's RGB frames with shape:\n",
    "            (channel, time, height, width).\n",
    "\n",
    "    Returns:\n",
    "        x (Tensor): Scaled tensor by divide 255.\n",
    "    \"\"\"\n",
    "    return x / 255.0\n",
    "\n",
    "def change_brightness(x,max_b=60):\n",
    "    \"\"\"\n",
    "    Randomly changes the brightness by some delta_b.\n",
    "    Args:\n",
    "        x: A tensor of the clip's  frames with shape:\n",
    "            (channel, time, height, width).\n",
    "        max_b: maximum value of intensity to add/subtract from the clip\n",
    "\n",
    "    Returns:\n",
    "        x_hat (Tensor): clip with modified brightness\n",
    "\n",
    "    \"\"\"\n",
    "    b = random.randint(-max_b,max_b)\n",
    "    x_hat = x+b\n",
    "    x_hat = x_hat.clip(0,255)\n",
    "    return x_hat\n",
    "\n",
    "def rgb2gray(x):\n",
    "    \"\"\"\n",
    "    Convert clip frames from RGB mode to GRAYSCALE mode.\n",
    "    Args:\n",
    "        x (Tensor): A tensor of the clip's RGB frames with shape:\n",
    "            (channel, time, height, width).\n",
    "\n",
    "    Returns:\n",
    "        x (Tensor): Converted tensor\n",
    "    \"\"\"\n",
    "    return x[[0], ...]\n",
    "\n",
    "\n",
    "def rgb2var(x,var_dim=1):\n",
    "    assert var_dim in [1,2]\n",
    "    gray = torch.squeeze(x[[0],...])\n",
    "    var = gray.var(axis=0).numpy()\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
    "    ekernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    opening = cv2.morphologyEx(var, cv2.MORPH_OPEN, kernel)\n",
    "    erode = cv2.erode(opening,ekernel,iterations=2)\n",
    "    dilate_var = torch.tensor(cv2.dilate(erode,kernel,iterations=10))\n",
    "    if var_dim==2:\n",
    "        var_array = torch.stack((gray,torch.stack([dilate_var]*gray.shape[0]),torch.stack([dilate_var]*gray.shape[0])))\n",
    "    elif var_dim==1:\n",
    "        var_array = torch.stack((gray, gray, torch.stack([dilate_var] * gray.shape[0])))\n",
    "    return var_array\n",
    "def Ptvfishbase(cfg, mode):\n",
    "    \"\"\"\n",
    "    Construct the Fishbase video loader with a directory, each directory is split into modes ('train', 'val', 'test')\n",
    "    and inside each mode are subdirectories for each label class.\n",
    "    For `train` and `val` mode, a single clip is randomly sampled from every video\n",
    "    with random cropping, scaling, and flipping. For `test` mode, multiple clips are\n",
    "    uniformaly sampled from every video with center cropping.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs.\n",
    "        mode (string): Options includes `train`, `val`, or `test` mode.\n",
    "            For the train and val mode, the data loader will take data\n",
    "            from the train or val set, and sample one clip per video.\n",
    "            For the test mode, the data loader will take data from test set,\n",
    "            and sample multiple clips per video.\n",
    "    \"\"\"\n",
    "    # Only support train, val, and test mode.\n",
    "    assert mode in [\n",
    "        \"train\",\n",
    "        \"val\",\n",
    "        \"test\",\n",
    "        'train_eval',\n",
    "        'val_eval',\n",
    "    ], \"Split '{}' not supported\".format(mode)\n",
    "\n",
    "    clip_duration = (\n",
    "        cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE / cfg.DATA.TARGET_FPS\n",
    "    )\n",
    "    path_to_dir = os.path.join(\n",
    "        cfg.DATA.PATH_TO_DATA_DIR, mode.split('_')[0] #added split to deal with the case of train_eval and val_eval\n",
    "    )\n",
    "\n",
    "    labeled_video_paths = LabeledVideoPaths.from_directory(path_to_dir)\n",
    "    num_videos = len(labeled_video_paths)\n",
    "    labeled_video_paths.path_prefix = cfg.DATA.PATH_PREFIX\n",
    "    if mode in [\"train\", \"val\"]:\n",
    "        num_clips = 1\n",
    "        num_crops = 1\n",
    "\n",
    "        transform = Compose(\n",
    "            [\n",
    "                ApplyTransformToKey(\n",
    "                    key=\"video\",\n",
    "                    transform=Compose(\n",
    "                        [\n",
    "                            UniformTemporalSubsample(cfg.DATA.NUM_FRAMES),\n",
    "                            Lambda(div255),\n",
    "                            RandomColorJitter(brightness_ratio=cfg.DATA.BRIGHTNESS_RATIO, p=cfg.DATA.BRIGHTNESS_PROB), #first trial 0.3\n",
    "                            RandomGaussianBlur(kernel=13, sigma=(6.0,10.0), p=cfg.DATA.BLUR_PROB), # first trial 0.2\n",
    "                            NormalizeVideo(cfg.DATA.MEAN, cfg.DATA.STD),\n",
    "                            ShortSideScale(cfg.DATA.TRAIN_JITTER_SCALES[0]),\n",
    "                        ]\n",
    "                        + (\n",
    "                            [Lambda(rgb2gray)]\n",
    "                            if cfg.DATA.INPUT_CHANNEL_NUM[0] == 1\n",
    "                            else []\n",
    "                        )\n",
    "                        + (\n",
    "                            [VarianceImageTransform(var_dim=cfg.DATA.VAR_DIM)]\n",
    "                            if cfg.DATA.VARIANCE_IMG\n",
    "                            else []\n",
    "                        )\n",
    "                        + (\n",
    "                            [RandomHorizontalFlipVideo(p=0.5),\n",
    "                             RandomVerticalFlipVideo(p=0.5),\n",
    "                             RandomRot90Video(p=0.5)]\n",
    "                            if cfg.DATA.RANDOM_FLIP\n",
    "                            else []\n",
    "                        )\n",
    "                        + [PackPathway(cfg)]\n",
    "                    ),\n",
    "                ),\n",
    "                DictToTuple(num_clips, num_crops),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        clip_sampler = make_clip_sampler(\"random\", clip_duration)\n",
    "        if cfg.NUM_GPUS > 1:\n",
    "            video_sampler = DistributedSampler\n",
    "        else:\n",
    "            video_sampler = (\n",
    "                RandomSampler if mode == \"train\" else SequentialSampler\n",
    "            )\n",
    "    else:\n",
    "        num_clips = cfg.TEST.NUM_ENSEMBLE_VIEWS\n",
    "        num_crops = cfg.TEST.NUM_SPATIAL_CROPS\n",
    "\n",
    "        transform = Compose(\n",
    "            [\n",
    "                ApplyTransformToKey(\n",
    "                    key=\"video\",\n",
    "                    transform=Compose(\n",
    "                        [\n",
    "                            UniformTemporalSubsample(cfg.DATA.NUM_FRAMES),\n",
    "                            Lambda(div255),\n",
    "                            NormalizeVideo(cfg.DATA.MEAN, cfg.DATA.STD),\n",
    "                            ShortSideScale(\n",
    "                                size=cfg.DATA.TRAIN_JITTER_SCALES[0]\n",
    "                            ),\n",
    "                        ]\n",
    "                        + (\n",
    "                            [Lambda(rgb2gray)]\n",
    "                            if cfg.DATA.INPUT_CHANNEL_NUM[0] == 1\n",
    "                            else []\n",
    "                        )\n",
    "                        + (\n",
    "                            [VarianceImageTransform(var_dim=cfg.DATA.VAR_DIM)]\n",
    "                            if cfg.DATA.VARIANCE_IMG\n",
    "                            else []\n",
    "                        )\n",
    "                    ),\n",
    "                ),\n",
    "                ApplyTransformToKey(key=\"video\", transform=PackPathway(cfg)),\n",
    "                DictToTuple(num_clips, num_crops),\n",
    "            ]\n",
    "        )\n",
    "        clip_sampler = make_clip_sampler(\n",
    "            \"constant_clips_per_video\",\n",
    "            clip_duration,\n",
    "            num_clips,\n",
    "            num_crops,\n",
    "        )\n",
    "        video_sampler = (\n",
    "            DistributedSampler if cfg.NUM_GPUS > 1 else SequentialSampler\n",
    "        )\n",
    "\n",
    "    return PTVDatasetWrapper(\n",
    "        num_videos=num_videos,\n",
    "        clips_per_video=num_clips,\n",
    "        crops_per_clip=num_crops,\n",
    "        dataset=LabeledVideoDataset(\n",
    "            labeled_video_paths=labeled_video_paths,\n",
    "            clip_sampler=clip_sampler,\n",
    "            video_sampler=video_sampler,\n",
    "            transform=transform,\n",
    "            decode_audio=False,\n",
    "        ),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef4e998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_loader(cfg, split, is_precise_bn=False):\n",
    "    \"\"\"\n",
    "    Constructs the data loader for the given dataset.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "        split (str): the split of the data loader. Options include `train`,\n",
    "            `val`, and `test`.\n",
    "    \"\"\"\n",
    "    assert split in [\"train\", \"val\", \"test\",\"train_eval\",\"val_eval\"]\n",
    "    if split in [\"train\"]:\n",
    "        dataset_name = cfg.TRAIN.DATASET\n",
    "        batch_size = int(cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS))\n",
    "        shuffle = True\n",
    "        drop_last = True\n",
    "    elif split in [\"val\", \"val_eval\",\"train_eval\"]:\n",
    "        dataset_name = cfg.TRAIN.DATASET\n",
    "        batch_size = int(cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS))\n",
    "        shuffle = False\n",
    "        drop_last = False\n",
    "    elif split in [\"test\"]:\n",
    "        dataset_name = cfg.TEST.DATASET\n",
    "        batch_size = int(cfg.TEST.BATCH_SIZE / max(1, cfg.NUM_GPUS))\n",
    "        shuffle = False\n",
    "        drop_last = False\n",
    "\n",
    "    # Construct the dataset\n",
    "    dataset = Ptvfishbase(cfg, split)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=cfg.DATA_LOADER.NUM_WORKERS,\n",
    "            pin_memory=cfg.DATA_LOADER.PIN_MEMORY,\n",
    "            drop_last=drop_last,\n",
    "            collate_fn=detection_collate if cfg.DETECTION.ENABLE else None,\n",
    "            worker_init_fn=utils.loader_worker_init_fn(dataset),\n",
    "        )\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "325fd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model,loader,cfg):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    y_hats = []\n",
    "    all_labels = []\n",
    "    stats = {}\n",
    "    running_err = 0\n",
    "    all_preds = []\n",
    "    all_file_names = []\n",
    "    file_list = loader.dataset.dataset._labeled_videos._paths_and_labels\n",
    "    with torch.no_grad():\n",
    "        for cur_iter, (inputs, labels, indices, meta) in enumerate(loader):\n",
    "            if isinstance(inputs, (list,)):\n",
    "                for i in range(len(inputs)):\n",
    "                    inputs[i] = inputs[i].cuda(non_blocking=True)\n",
    "            else:\n",
    "                inputs = inputs.cuda(non_blocking=True)\n",
    "            labels = labels.cuda()\n",
    "            preds = model(inputs)\n",
    "            preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "            file_names = [file_list[f][0] for f in indices]\n",
    "            all_file_names+=file_names\n",
    "            y_hat = preds.max(axis=1).indices\n",
    "            all_preds.append(preds)\n",
    "            y_hats.append(y_hat)\n",
    "            all_labels.append(labels)\n",
    "            num_correct += (labels == y_hat).sum()\n",
    "            k = min(cfg.MODEL.NUM_CLASSES, 5)  # in case there aren't at least 5 classes in the dataset\n",
    "            num_topks_correct = metrics.topks_correct(preds, labels, (1, k))\n",
    "            # Combine the errors across the GPUs.\n",
    "            top1_err, _ = [\n",
    "                (1.0 - x / preds.size(0)) * 100.0 for x in num_topks_correct\n",
    "            ]\n",
    "            top1_err = top1_err.item()\n",
    "            running_err += top1_err*preds.size(0)\n",
    "        all_labels = torch.hstack(all_labels)\n",
    "        y_hats = torch.hstack(y_hats)\n",
    "        all_preds = torch.vstack(all_preds)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(1-all_labels.cpu(),1-y_hats.cpu()).ravel()\n",
    "        stats['fns'] = fn\n",
    "        stats['fps'] = fp\n",
    "        stats['tns'] = tn\n",
    "        stats['tps'] = tp\n",
    "        stats['top1_err'] = (running_err/len(loader.dataset))\n",
    "        stats['accuracy'] = (num_correct/len(loader.dataset)).item()\n",
    "    return all_labels, y_hats, stats,all_preds, all_file_names\n",
    "\n",
    "\n",
    "def train_one_epoch(model,optim,loader_train,loss_func):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    train_stats = {'fps':0,'tns':0,'fns':0,'tps':0,'accuracy':0}\n",
    "    y_hats = []\n",
    "    all_labels = []\n",
    "    running_err = 0\n",
    "    for cur_iter, (inputs, labels, _, meta) in enumerate(loader_train):\n",
    "        if isinstance(inputs, (list,)):\n",
    "            for i in range(len(inputs)):\n",
    "                inputs[i] = inputs[i].cuda(non_blocking=True)\n",
    "        else:\n",
    "            inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda()\n",
    "        preds = model(inputs)\n",
    "        preds = torch.nn.functional.softmax(preds,dim=1)\n",
    "        y_hat = preds.max(axis=1).indices\n",
    "        y_hats.append(y_hat)\n",
    "        all_labels.append(labels)\n",
    "        num_correct += (labels==y_hat).sum()\n",
    "        loss = loss_func(preds,labels)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        k = min(cfg.MODEL.NUM_CLASSES, 5)  # in case there aren't at least 5 classes in the dataset\n",
    "        num_topks_correct = metrics.topks_correct(preds, labels, (1, k))\n",
    "        top1_err, _ = [\n",
    "            (1.0 - x / preds.size(0)) * 100.0 for x in num_topks_correct]\n",
    "        running_err += top1_err * preds.size(0)\n",
    "        train_loss += loss/preds.shape[0]\n",
    "    all_labels = torch.hstack(all_labels)\n",
    "    y_hats = torch.stack(y_hats).ravel()\n",
    "    tn, fp, fn, tp = confusion_matrix(1 - all_labels.cpu(),\n",
    "                                      1 - y_hats.cpu()).ravel()  # since feed is label 0 and we want it as label 1,\n",
    "    train_stats['loss'] = train_loss\n",
    "    train_stats['fns'] = fn\n",
    "    train_stats['fps'] = fp\n",
    "    train_stats['tns'] = tn\n",
    "    train_stats['tps'] = tp\n",
    "    train_stats['top1_err'] = (running_err / len(loader_train.dataset))\n",
    "    train_stats['accuracy'] = num_correct / len(loader_train.dataset)\n",
    "    return model, optim, train_stats\n",
    "\n",
    "def train(cfg, pretrained=True):\n",
    "    print('starting train')\n",
    "    np.random.seed(cfg.RNG_SEED)\n",
    "    torch.manual_seed(cfg.RNG_SEED)\n",
    "    loader_train = construct_loader(cfg, 'train')\n",
    "    loader_val = construct_loader(cfg, 'val')\n",
    "    model_name = \"slowfast_r50\"\n",
    "    model = torch.hub.load(\"facebookresearch/pytorchvideo:main\", model=model_name, pretrained=pretrained)\n",
    "    model.blocks[6].proj = torch.nn.Linear(in_features=2304, out_features=cfg.MODEL.NUM_CLASSES)\n",
    "    model.cuda()\n",
    "    if cfg.TENSORBOARD.ENABLE:\n",
    "        writer = tb.TensorboardWriter(cfg)\n",
    "    else:\n",
    "        writer = None\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = cfg.SOLVER.MOMENTUM)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    loss_func = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    exp_dir = cfg.OUTPUT_DIR\n",
    "    os.makedirs(os.path.join(exp_dir,'checkpoints'), exist_ok=True)\n",
    "\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    train_recall = []\n",
    "    val_recall = []\n",
    "    train_losses = []\n",
    "    prec_func = lambda st: st['tps'] / (st['tps'] + st['fps'])\n",
    "    rec_func = lambda st: st['tps'] / (st['tps'] + st['fns'])\n",
    "    f1_func = lambda st: 2 * (st['precision'] * st['recall']) / (st['precision'] + st['recall'])\n",
    "    for cur_epoch in range(cfg.SOLVER.MAX_EPOCH):\n",
    "        loader.shuffle_dataset(loader_train, cur_epoch)\n",
    "        model, optimizer, train_stats = train_one_epoch(model, optimizer, loader_train, loss_func)\n",
    "        scheduler.step(train_stats['loss'])\n",
    "        train_labels,train_y_hats,train_eval_stats,_,_ = eval_epoch(model,loader_train,cfg)\n",
    "        val_labels,val_y_hats,val_stats,_,_ = eval_epoch(model,loader_val,cfg)\n",
    "        train_stats['precision'] = prec_func(train_stats)\n",
    "        train_stats['recall'] = rec_func(train_stats)\n",
    "        train_stats['f1'] = f1_func(train_stats)\n",
    "        train_eval_stats['precision'] = prec_func(train_eval_stats)\n",
    "        train_eval_stats['recall'] = rec_func(train_eval_stats)\n",
    "        train_eval_stats['f1'] = f1_func(train_eval_stats)\n",
    "        val_stats['precision'] = prec_func(val_stats)\n",
    "        val_stats['recall'] = rec_func(val_stats)\n",
    "        val_stats['f1'] = f1_func(val_stats)\n",
    "        train_f1s.append(train_stats['f1'])\n",
    "        val_f1s.append(val_stats['f1'])\n",
    "        train_recall.append(train_stats['recall'])\n",
    "        val_recall.append(val_stats['recall'])\n",
    "        train_losses.append(train_stats['loss'])\n",
    "        if writer is not None:\n",
    "            writer.add_scalars(\n",
    "                {\n",
    "                    \"Train/epoch_loss\": train_stats['loss'],\n",
    "                    \"Train/epoch_top1_err\": train_stats['top1_err'],\n",
    "                    \"Train_eval/epoch_top1_err\": train_eval_stats['top1_err'],\n",
    "                    \"Train_eval/epoch_accuracy\": train_eval_stats['accuracy'],\n",
    "                    \"Train_eval/epoch_precision\": train_eval_stats['precision'],\n",
    "                    \"Train_eval/epoch_recall\": train_eval_stats['recall'],\n",
    "                    \"Val/epoch_top1_err\": val_stats['top1_err'],\n",
    "                    \"Val/epoch_precision\": train_eval_stats['precision'],\n",
    "                    \"Val/epoch_recall\": train_eval_stats['recall']\n",
    "                },\n",
    "                global_step=cur_epoch,\n",
    "            )\n",
    "        print(f'{cur_epoch}/{cfg.SOLVER.MAX_EPOCH}: loss {train_stats[\"loss\"]} '\n",
    "              f'Train F1 {train_stats[\"f1\"]:.2f}, acc {train_stats[\"accuracy\"]:.2f}, '\n",
    "              f'recall {train_stats[\"recall\"]:.2f}')\n",
    "        print(f'Val F1 {val_stats[\"f1\"]:.2f}, acc {val_stats[\"accuracy\"]:.2f}, recall {val_stats[\"recall\"]:.2f}')\n",
    "        torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(),\n",
    "                    'train_losses':train_losses,\n",
    "                    'train_labels': train_labels,\n",
    "                    'train_y_hats':train_y_hats,\n",
    "                    'val_labels':val_labels,\n",
    "                    'val_y_hats':val_y_hats,\n",
    "                    'scheduler_state': scheduler.state_dict(),\n",
    "                    'train_stats': train_stats,\n",
    "                    'train_eval_stats':train_eval_stats,\n",
    "                    'val_stats': val_stats},\n",
    "                   os.path.join(exp_dir, 'checkpoints',f'pretrained_epoch{cur_epoch}.pt'))\n",
    "    if writer is not None:\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54652d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/shirbar/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50: loss 3.4249308109283447 Train F1 0.10, acc 0.53, recall 0.05\n",
      "Val F1 0.50, acc 0.64, recall 0.36\n",
      "1/50: loss 3.3388326168060303 Train F1 0.54, acc 0.66, recall 0.41\n",
      "Val F1 0.62, acc 0.55, recall 0.73\n",
      "2/50: loss 3.1456546783447266 Train F1 0.72, acc 0.78, recall 0.59\n",
      "Val F1 0.69, acc 0.59, recall 0.91\n",
      "3/50: loss 2.957608222961426 Train F1 0.80, acc 0.81, recall 0.77\n",
      "Val F1 0.74, acc 0.68, recall 0.91\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_884141/722565448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#cfg.DATA.PATH_TO_DATA_DIR =  '/home/shirbar/data/strike_ds_dist_cropped_equal'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#cfg.OUTPUT_DIR = '/mnt/slowfast_results/pretrained_sgd_ablation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_884141/3753249840.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cfg, pretrained)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOLVER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y_hats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_eval_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_884141/3753249840.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optim, loader_train, loss_func)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mrunning_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcur_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchvideo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchvideo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchvideo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchvideo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchvideo/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchvideo/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self,cfg_file):\n",
    "        self.cfg_file = cfg_file\n",
    "        self.shard_id = 0\n",
    "        self.num_shards = 1\n",
    "        self.init_method = 'tcp://localhost:9999'\n",
    "        self.opts = None\n",
    "        \n",
    "args = Args('/media/shirbar/DATA/codes/SlowFast/configs/FishBase/SLOWFAST_8x8_R50_feed_pretrained.yaml')\n",
    "cfg = load_config(args)\n",
    "cfg = assert_and_infer_cfg(cfg)\n",
    "#cfg.DATA.PATH_TO_DATA_DIR =  '/home/shirbar/data/strike_ds_dist_cropped_equal' \n",
    "#cfg.OUTPUT_DIR = '/mnt/slowfast_results/pretrained_sgd_ablation'\n",
    "train(cfg, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6cdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
